{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;**Yelp Data Challenge**\n",
    " - &ensp;&ensp;**Restaurants Classification by sentiment analysis of user reviews**<br>\n",
    "\n",
    "     &ensp;&ensp; By&ensp;&ensp;Manoj Vaidya<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ yelp_academic_dataset_business DATASET\n",
    "\n",
    "business = pd.read_csv(\"yelp_academic_dataset_business.csv\")\n",
    "print(business.shape)\n",
    "business.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop attributes which are not required\n",
    "business.drop('neighborhood', axis = 1, inplace = True)\n",
    "business.drop('postal_code', axis = 1, inplace = True)\n",
    "business.drop('attributes', axis = 1, inplace = True)\n",
    "business.drop('hours', axis = 1, inplace = True)\n",
    "business.drop(business[business.is_open == 0].index, inplace = True)\n",
    "business.drop('is_open', axis = 1, inplace = True)\n",
    "business.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business=business.dropna(subset=['categories'])\n",
    "business=business.dropna(subset=['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill null address as not_known\n",
    "\n",
    "business['address'].fillna(\"not_known\", inplace=True)\n",
    "\n",
    "#Fill latitude and longitude with city means\n",
    "\n",
    "if business[\"latitude\"].isna:\n",
    "    business[\"latitude\"] = business.groupby(\"city\").transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "if business[\"longitude\"].isna:\n",
    "    business[\"longitude\"] = business.groupby(\"city\").transform(lambda x: x.fillna(x.mean()))[\"longitude\"]\n",
    "\n",
    "print(business.isnull().sum())    \n",
    "business.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categories other than restaurants is not necesaary. Can be droped\n",
    "\n",
    "business.drop(business[~business['categories'].str.contains('Restaurants')].index, inplace = True)\n",
    "print(business.shape)\n",
    "business.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ yelp_academic_dataset_review DATASET\n",
    "\n",
    "review = pd.read_csv(\"yelp_academic_dataset_review.csv\")\n",
    "print(review.shape)\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null reviews can be dropped\n",
    "\n",
    "print(review.isnull().sum())\n",
    "review=review.dropna()\n",
    "review.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting day,month and year from date\n",
    "\n",
    "review['date'] = pd.to_datetime(review.date, format='%Y-%m-%d', errors='ignore')\n",
    "review['day'] = review['date'].dt.day\n",
    "review['month'] = review['date'].dt.month\n",
    "review['year'] = review['date'].dt.year\n",
    "review.drop('date', axis = 1, inplace = True)\n",
    "review.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename stars to avoid conflict with other dataset\n",
    "\n",
    "review = review.rename(index=str,columns={'stars':'review_stars'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##MERGE BUSINESS AND REVIEW DATA\n",
    "\n",
    "business_reviews=business.merge(review, left_on='business_id', right_on='business_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Reduction (To avoid memory overhead during text processing)\n",
    "\n",
    "business_review_red=business_review.sample(frac=0.60, random_state=99)\n",
    "business_review_red=business_review_red.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count\n",
    "business_review_red['word_count(review)'] = business_review_red['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "#char_count\n",
    "business_review_red['char_count(review)'] = business_review_red['text'].str.len() ## this also includes spaces\n",
    "\n",
    "#average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "business_review_red['avg_word_len(review)'] = business_review_red['text'].apply(lambda x: avg_word(x))\n",
    "\n",
    "#stopwords\n",
    "stop = stopwords.words('english')\n",
    "business_review_red['stopwords(review)'] = business_review_red['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "\n",
    "\n",
    "business_review_red[['text','word_count(review)','char_count(review)','avg_word_len(review)','stopwords(review)']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uppercase to lowercase\n",
    "\n",
    "business_review_red['text'] = business_review_red['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "print(\"Converted.....\")\n",
    "\n",
    "\n",
    "#removing punctuation\n",
    "\n",
    "business_review_red['text'] =business_review_red['text'].str.replace('[^\\w\\s\\-\\+\\-]','')\n",
    "print(\"Removed punctuations....\")\n",
    "\n",
    "\n",
    "#remove stop_words\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "business_review_red['text'] =business_review_red['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(\"Removed stop words.....\")\n",
    "\n",
    "\n",
    "#spelling correction\n",
    "\n",
    "business_review_red['text'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "print(\"Spellings corrected.....\")\n",
    "\n",
    "\n",
    "business_review_red['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "\n",
    "print(\"Performing Lemmatization.....\")\n",
    "business_review_red['text'] = business_review_red['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "print(\"Lemmatization Done......\")\n",
    "\n",
    "\n",
    "\n",
    "#Tokenization\n",
    "\n",
    "print(\"Performing Tokenization.......\")\n",
    "business_review_red['text']=business_review_red.apply(lambda row: nltk.word_tokenize(row['text']),axis=1)\n",
    "print(\"tokenizaton done.......\")\n",
    "business_review_red['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename text to review to avoid conflicts\n",
    "\n",
    "business_review_red=business_review_red.rename(index=str,columns={'text':'review'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ yelp_academic_dataset_tip DATASET\n",
    "\n",
    "tip = pd.read_csv(\"yelp_academic_dataset_tip.csv\")\n",
    "print(tip.shape)\n",
    "tip.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null values and date attribute\n",
    "\n",
    "tip=tip.dropna()\n",
    "tip.drop('date', axis = 1, inplace = True)\n",
    "tip.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge business and tip  using business_id as key\n",
    "\n",
    "business_tip=business.merge(tip, left_on=['business_id'], right_on='business_id', how='left')\n",
    "print(business_tip.shape)\n",
    "business_tip.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows with no tips\n",
    "\n",
    "business_tip=business_tip.dropna()\n",
    "business_tip.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count\n",
    "business_tip['word_count(tip)'] = business_tip['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "\n",
    "#char_count\n",
    "business_tip['char_count(tip)'] = business_tip['text'].str.len() ## this also includes spaces\n",
    "\n",
    "#average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/(len(words)+0.1))\n",
    "\n",
    "business_tip['avg_word_len(tip)'] = business_tip['text'].apply(lambda x: avg_word(x))\n",
    "\n",
    "\n",
    "#stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "business_tip['stopwords(tip)'] = business_tip['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "business_tip[['text','word_count(tip)','char_count(tip)','avg_word_len(tip)','stopwords(tip)']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uppercase to lowercase\n",
    "\n",
    "business_tip['text'] = business_tip['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "print(\"Converted.....\")\n",
    "\n",
    "\n",
    "#removing punctuation\n",
    "\n",
    "business_tip['text'] = business_tip['text'].str.replace('[^\\w\\s\\-\\+\\-]','')\n",
    "print(\"Punctuations removed....\")\n",
    "\n",
    "\n",
    "#remove stop_words\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "business_tip['text'] = business_tip['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "print(\"Stop words removed....\")\n",
    "\n",
    "\n",
    "#spelling correction\n",
    "\n",
    "business_tip['text'][:5].apply(lambda x: str(TextBlob(x).correct()))\n",
    "print(\"Spellings corrected....\")\n",
    "\n",
    "\n",
    "business_tip['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization\n",
    "\n",
    "print(\"Performing lemmatization....\")\n",
    "business_tip['text'] = business_tip['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "print(\"Lemmatization done........\")\n",
    "\n",
    "\n",
    "#Tokenization\n",
    "\n",
    "print(\"Performing tokenization.....\")\n",
    "business_tip['text']=business_tip.apply(lambda row: nltk.word_tokenize(row['text']),axis=1)\n",
    "print(\"Tokenization done....\")\n",
    "business_tip['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename text attribute to tip\n",
    "\n",
    "business_tip=business_tip.rename(index=str,columns={'text':'tip'})\n",
    "business_review_red.to_csv(\"business_tip.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_tip.to_csv(\"business_tip.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Action: \n",
    "## Loading dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import nltk\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import decomposition, grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_reviews = pd.read_csv(\"business_review_red(lat).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Clean the text data in review dataset\n",
    "## 2. Tokenize all text to sentences and then words\n",
    "## 3. Tag all the words with position tags\n",
    "## 4. Convert all tags to wordnet tags\n",
    "## 5. Lemmatize the words and create synsets\n",
    "## 6. Compute sentiment score (positive score - negative score)\n",
    "## 7. Return a sentiment polarity score: 1 = positive, 0 = negative\n",
    "\n",
    "#nltk.download('punkt')\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "  \n",
    "def convert_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    " \n",
    "def clean_text(text):\n",
    "    text = text.replace(\"<br />\", \" \")\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    #emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    #text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    #text = text.decode(\"utf-8\")\n",
    "    return text\n",
    " \n",
    "def compute_swn_polarity_score(text):\n",
    "    sentiment_score = 0.0\n",
    "    num_tokens = 0\n",
    "    text = clean_text(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        tagged_sentence = pos_tag(word_tokenize(sentence))\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = convert_tag(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment_score += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            num_tokens += 1\n",
    "    # sum greater than 0 => positive sentiment\n",
    "    if sentiment_score >= 0:\n",
    "        return 1\n",
    "    # All other scenarios => negative sentiment\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column 'polarity' with sentiment polarity as 1 (positive) or 0 (negative)\n",
    "business_reviews['polarity'] = business_reviews['text'].apply(compute_swn_polarity_score)\n",
    "business_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the net positive sentiment score: net_positive_sentiment_score = sum(polarity) / total_count_of_reviews\n",
    "\n",
    "def transform_review(business_reviews1):\n",
    "    business_reviews1_f = business_reviews1.groupby('business_id').mean().reset_index()\n",
    "    #review3['stars'].round(2)\n",
    "    business_reviews1_f['stars'] =  business_reviews1_f['stars'].apply(lambda x: round(x,2))\n",
    "    business_reviews1_f['polarity'] = business_reviews1_f['polarity'].apply(lambda x: round(x,2))\n",
    "    business_reviews1_f.rename(columns={'polarity':'net_positive_sentiment_score'}, inplace=True)\n",
    "    return  business_reviews1_f\n",
    "business_reviews1_f = transform_review( business_reviews1)\n",
    "business_reviews1_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_reviews1_f.drop(business_reviews1_f.columns[[5, 6,7, 8, 9, 10, 11, 12, 13, 14, 15]],axis=1,inplace=True)\n",
    "business_reviews1_f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip = pd.read_csv(\"tip_pre_proc.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column 'polarity' with sentiment polarity as 1 (positive) or 0 (negative)\n",
    "\n",
    "tip['polarity'] = tip2['text'].apply(compute_swn_polarity_score)\n",
    "tip['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the net positive sentiment score: net_positive_sentiment_score = sum(polarity) / total_count_of_reviews\n",
    "\n",
    "def transform_tip(tip):\n",
    "    tip1 = tip.groupby('business_id').mean().reset_index()\n",
    "    tip1['polarity'] = tip1['polarity'].apply(lambda x: round(x,2))\n",
    "    tip1.rename(columns={'polarity':'net_positive_sentiment_score'}, inplace=True)\n",
    "    return tip1\n",
    "\n",
    "tip1 = transform_tip(tip)\n",
    "tip1['net_positive_sentiment_score'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Join business_review and tip1 dataframes\n",
    "\n",
    "def merge_business_review_tip(tip1, business_review):\n",
    "    business_review_tip = pd.merge(business_review, tip1, on='business_id', how='left')\n",
    "    business_review_tip.rename(columns={'net_positive_sentiment_score_x':'net_positive_sentiment_score_review'}, inplace=True)\n",
    "    business_review_tip.rename(columns={'net_positive_sentiment_score_y':'net_positive_sentiment_score_tip'}, inplace=True)\n",
    "    return business_review_tip\n",
    "\n",
    "business_review_tip = merge_business_checkin_review_tip(tip1, business_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = business_checkin_review_tip\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.heatmap(corr, mask=mask, vmax=1, annot=True,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[[7]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"business_review(senti_final2222).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Categorize all restaurants\n",
    "\n",
    "def categorize(data):\n",
    "    if data['stars'] >= 3.8:\n",
    "        return 'Excellent'\n",
    "    elif data['stars'] <= 1.2:\n",
    "        return 'Poor'\n",
    "    else:\n",
    "        return 'Average'\n",
    "    \n",
    "data['restaurant_category'] = data.apply(categorize, axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data[['business_id', 'restaurant_category','net_positive_sentiment_score']]\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=pd.read_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting into test and train\n",
    "\n",
    "X = final_data[['net_positive_sentiment_score']]\n",
    "y = final_data[['restaurant_category']].values.ravel()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(max_depth = 5) \n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "classifiers.append('DecisionTreeClassifier')\n",
    "accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel = 'rbf', C = 10) \n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "classifiers.append('svm.SVC')\n",
    "accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB() \n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "classifiers.append('GaussianNB')\n",
    "accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "classifiers.append('LogisticRegression')\n",
    "accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "classifiers.append('KNN')\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finding optimal number of neighbors\n",
    "\n",
    "l=list(range(1,50))\n",
    "a=pd.Series()\n",
    "x=[5,10,15,20,25,30,35,40,45]\n",
    "for i in l:\n",
    "    clf = KNeighborsClassifier(n_neighbors=i) \n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    a = a.append(pd.Series(accuracy_score(pred, y_test)))\n",
    "plt.title('Find optimum number of neighbors')\n",
    "plt.plot(l, a)\n",
    "plt.xticks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 25)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "classifiers.append('KNeighborsClassifier(OPTIMIZED)')\n",
    "accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 27)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "#classifiers.append('KNeighborsClassifier')\n",
    "#accuracies.append(accuracy)\n",
    "print(round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree with GridSearchCV\n",
    "from sklearn import tree, grid_search\n",
    "parameters = {'criterion':('gini', 'entropy'), 'max_depth':[16, 37, 58, 100, 15]}\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "clf = grid_search.GridSearchCV(dt, parameters)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(pred, y_test)\n",
    "print(round(accuracy, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid Search - Used to find best combination of parameters\n",
    "XGB_model = xgb.XGBClassifier(objective='multi:softprob',subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "\n",
    "param_grid = {'max_depth': [10,20,30,40,50,60,70,80,90], 'learning_rate': [0.1, 0.3], 'n_estimators': [25, 50]}.\n",
    "model = grid_search.GridSearchCV(estimator=XGB_model, param_grid=param_grid,scoring='accuracy', verbose=1, n_jobs=1, iid=True, refit=True, cv=3)\n",
    "\n",
    "#model.fit(X, y)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "classifiers.append('DecisionTreeClassifier(XGboost)')\n",
    "accuracies.append(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sns.set_context(\"paper\", font_scale=1.6)\n",
    "plt.scatter(classifiers, accuracies, s=600, c=\"green\", alpha=0.5)\n",
    "plt.title('Accuracy vs Classifiers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X)\n",
    "data['predicted_restaurant_category'] = preds\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Poor', 'Average', 'Excellent']\n",
    "y1 = len(data[data.restaurant_category == 'Poor'])\n",
    "y2 = len(data[data.restaurant_category == 'Average'])\n",
    "y3 = len(data[data.restaurant_category == 'Excellent'])\n",
    "y1_pred = len(data[data.predicted_restaurant_category == 'Poor'])\n",
    "y2_pred = len(data[data.predicted_restaurant_category == 'Average'])\n",
    "y3_pred = len(data[data.predicted_restaurant_category == 'Excellent'])\n",
    "y = [y1, y2, y3]\n",
    "y_pred = [y1_pred, y2_pred, y3_pred]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.set_context(\"paper\", font_scale=1.3)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.subplot(1,2,1)\n",
    "sns.barplot(x=x, y=y, data=data, alpha = 0.7)\n",
    "plt.subplot(1,2,2)\n",
    "sns.barplot(x=x, y=y_pred, data=data, alpha = 0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
